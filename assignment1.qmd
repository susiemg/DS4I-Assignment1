---
title: "STA5073Z Assignment 1"
author: "Susana Maganga"
format: html
editor: visual
toc: true
number-sections: true
embed-resources: true
execute: 
  echo: false
---

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
# Load packages
library(tidyverse)
library(ggplot2)
library(caret)
library(rpart)
library(tidytext)
library(kableExtra)
library(keras)
library(tfhub)
library(rpart.plot)

# Load data
load("sona.RData")

# Remove date appearing at the beginning of speech
x <- sona$speech
y <- sub('^\\w*\\s*\\w*\\s*\\w*\\s*', '', x[1:34])
sona$speech[1:34] <- y

# Format 35th speech 
z <- sub("^[A-Za-z]+, \\d{1,2} [A-Za-z]+ \\d{4}  ", "", x[35])
sona$speech[35] <- z

# Format 36th speech
a <- sub("\\d{1,2} [A-Za-z]+ \\d{4}", "", x[36])
sona$speech[36] <- a

# Remove all non-alphanumeric characters
sona$speech <- str_replace_all(sona$speech, "[^[:alnum:]]", " ")

# Convert to appropriate format
sona$president_13 <- as.factor(sona$president_13)
sona$year <- as.numeric(sona$year)
sona$date <- as.Date(sona$date,format = "%d-%m-%Y")
tidy_sona <- sona %>% unnest_tokens(word, speech, token = "words", to_lower = T)
```

## Introduction

A State of Nation Address (SONA) is a speech given by the President of South Africa which serves as an annual report on the nation's state. The aim is to inform the public of various topics such the nation's progress, challenges, economy, budget, legislative proposals and future plans.

Presidents have different priorities, themes and legacies when they come into office. At times, it may be of interest to know the individual goals of presidents. The objective of this assignment is therefore to build a predictive model that predicts the president that made a particular statement. Several predictive models built based on varied input data will be built and assessed for performance.

## Data and Methods

### Data Collection

The data used for this assignment were SONA speeches delivered by South African presidents from 1994 to 2023. The speeches include those delivered by: FW de Klerk, Nelson Mandela, Thabo Mbeki, Kgalema Motlanthe, Jacob Zuma and Cyril Ramaphosa, who is the current president. There is a total of 36 speeches in the dataset, which were sourced from the South African Government website (South African Government, n.d.).

### Data Cleaning and Preprocessing

Prior to beginning the analysis, the data had to be cleaned and preprocessed. Firstly, all the text files containing the speeches were read into R and combined into a single dataframe. Columns containing the date, year and president's name were appended to the data. Thereafter, the speeches were cleaned to remove the dates that appeared in the first line and any punctuation included in the speech. Finally, the columns were set to their appropriate formats i.e. date and factor.

The dataset was then converted into a "tidy" format where the speeches were tokenised. Tokenisation is a process where a column, in this case the speech, is split into a specified *token* which include, but are not limited to, a word, sentence or n-gram. An n-gram is a sequence of n words that are adjacent to each other in a text. The words were also converted to lower-case.

### Exploratory Data Analysis

To gain insight on the data, exploratory data analysis was conducted. A table with the number of speeches made by each president is displayed below. It can be seen that deKlerk and Motlanthe have only one speech. This can be attributed to the fact that deKlerk's term ended in 1994 and the data available ranges from the years 1994 to 2023. Motlanthe only served for approximately seven months as he was an interim president, after the resignation of Thabo Mbeki.

```{r Speech per President, warning=FALSE, message=FALSE, echo=FALSE}
speech_count_per_president <- sona %>%
  group_by(president_13) %>%
  summarize(speech_count = n())

kbl(speech_count_per_president, col.names = c("President", "Count"), caption = "Number of Speeches by President") %>% kable_styling(bootstrap_options = c("hover","striped"),latex_options = "HOLD_position")
```

The total number of words by each president was also calculated. Thabo Mbeki had the most words, with a value of 84,086. Similar to the results in the table, deKlerk and Motlanthe had the least number of words, with 2,337 and 8,679 words, respectively.

```{r Words per President, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center', fig.show="hold"}
word_count_per_president <- sona %>%
  mutate(word_count = lengths(strsplit(speech, " "))) %>%
  group_by(president_13) %>%
  summarize(total_words = sum(word_count))

ggplot(word_count_per_president, aes(x = president_13, y = total_words, fill = president_13)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Words per President",
       x = "President",
       y = "Word Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  scale_fill_discrete(name = "President")
```

The most commonly used words overall and broken down by president are displayed below. This was done after excluding stop words. Stop words are a set of commonly used words in a language such as "the", "and" and "is". Not surprisingly, the most prominent words were generally **government**, **people** and **south**. deKlerk was different, having **constitution** and **freedom** as his most spoken words, in addition to **south**.

```{r Top 20 Words Used Excl Stop, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center', fig.show="hold"}
sona %>% 
  unnest_tokens(word, speech, token = 'words') %>%
  count(word, sort=TRUE) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(rank(desc(n)) <= 20) %>% 
  ggplot(aes(x = reorder(word, n), y = n)) + geom_col(fill="lightblue") + coord_flip() + xlab("") + ylab("Count") + labs(title="Top 20 Most Used Words (Excluding Stop Words)")
```

```{r Top 5 Words Used per President, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center', fig.show="hold"}
top_pres <- tidy_sona %>%
  count(president_13, word) %>%
  filter(!word %in% stop_words$word) %>%
  group_by(president_13) %>%
  filter(rank(desc(n)) <= 5)

top_pres %>%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(fill = "skyblue") +
  facet_wrap(~president_13, scales = "free") +
  coord_flip() +
  labs(title = "Top 5 Positive Words per President", x = "", y = "Frequency")
```
### Feature Representation

Before the predictive models can be fit, the text needs to be put in a format that can be fed into the algorithm. Three formats were considered: Bag-of-Words, Term frequency-inverse-document-frequency (tf-idf) and word embeddings.

#### Bag-of-words

The bag-of-words model is a representation of text where the unique words present in the dataset (corpus) are the "bag-of-words" or vocabulary. The frequency of occurrence of each word is used as features to train a classifier in the form of a matrix. The order of the words and grammar is discarded in this process. In this assignment, stop words were excluded from the dataset and the top 200 words were considered when using the bag-of-words model.

#### tf-idf

tf-idf is an approach where terms (words) present in a dataset are assigned a weight, where frequently occurring words are downweighted (such as stop words). Two components are calculated:

-   **tf**: The relative frequency of a word, i.e. the number of times a word appears in a particular document divided by the total number of words in the entire dataset.

-   **idf**: A measure of how many documents in the corpus contain a particular word. The inverse frequency then undergoes a log transformation. Commonly used words have their weights decreased more than those that are less frequently occurring.

The tf-idf score is calculated by multiplying the tf and idf components for each term.

#### Trigrams

An n-gram is a sequence of n words in a text. Trigrams, specifically, are sequences of 3 words adjacent to each other in text. These are more contextual as they have a certain order to them. These were also explored in this analysis. The trigrams were first extracted from each speech, separated and stop words were removed. After this was done, the trigrams were put back together and the top 200 trigrams were extracted.

#### Word Embeddings

Finally, word embeddings were considered. This is an approach where tokenisation turns each text (in this case, speech) into a a vector of integers, where each integer represents a specified token. The default token is words, and that is what will be used in this assignment. When the vectors are generated, there is bound to be differences in length. For this reason, padding is introduced; this is where shorter sequences are appended with zeros.  

### Decision Trees

A decision tree is a non-parametric supervised learning algorithm, which can be used for classification and regression problems. The tree starts at the root node, which has no incoming branches; it then branches off into internal/decision nodes. Leaf nodes are all the possible outcomes within the dataset. A greedy search is done to identify optimal split points in the tree in a top-down manner until all observations have been classified. Information gain and Gini impurity are typically used as the splitting criterion. Information gain is the difference in entropy before and after a split on a particular attribute. The ideal split is the one that yields the highest impurity gain. Entropy is a measure of impurity of the sample values and values can fall between 0 and 1. If all samples are categorised in one class, entropy is 0.

$$Entropy(S)=-\sum_{c\in C}p(c)\,log_2p(c)$$

where:

-   $S$ is the dataset that entropy is calculated

-   $c$ is the classes in dataset $S$

-   $p(c)$ is the proportion of data points belonging to class $c$ to the number of total data points in set $S$

$$Information\;Gain(S,\alpha) = Entropy (S)-\sum_{v \in values(\alpha)} \frac{|S_v|}{|S|} Entropy(S_v)$$

where:

-   $\alpha$ represents a specific class label

-   $Entropy(S)$ is the entropy of dataset $S$

-   $\frac{|S_v|}{|S|}$ represents the proportion of the values in $|S_v|$ to the number of values in dataset $S$

Gini impurity is the probability of classifying a random data point incorrectly if it were labelled based on the distribution of the class distribution. A lower value indicates "purity", where a node mostly contains observations belonging to one class.

$$Gini\;Impurity = 1 - \sum_{i}(p_i)^2$$

where $p_i$ is the probability of an observation being placed in class $i$ (IBM, n.d.).

### Neural Networks

A Neural Network (NN) is a deep learning algorithm that is built to mimick the way that biological neurons in the brain signal to one another. The model consists of the input, hidden and output layers, each with nodes/neurons. The nodes have assigned weights and threshold values and are connected to nodes in other layers. A variable's importance is determined by weights, where larger weights have a more significant impact on the output. Inputs are multiplied by the corresponding node weights and summed; thereafter an output is generated by transforming the result through some activation function. Data is only transmitted to the subsequent layer if the output is above a specified threshold. Through gradient descent, the weights and threshold are adjusted gradually by minimising the cost function (IBM, n.d.).

There are various hyperparameters that can be tuned in an NN model to optimise its performance. In this particular NN model, the following hyperparameters were set:

-   Hidden layer structure: This comprises of number of hidden layers and nodes within each layer. The models used in this assignment had 1 hidden layer with 32 nodes.

-   Number of epochs: This is the number of times that the model iterates over the training dataset; each epoch consists of forward propagation, backward propagation and weight updates. The number of epochs was set to 65.

-   Dropout: Dropout is a regularisation technique that excludes random portions of the nodes during training to prevent overfitting. It specifies the proportion of random nodes that should be excluded during training. This reduces redundancy and reliance on some nodes. A value too low would have minimal effect, however a value too high would cause under-learning. A dropout ratio of 0.2 was used in the models.

-   Activation function: Activation functions facilitate non-linearity in neural networks. Common activation functions include sigmoid, tanh and recified linear unit (ReLU). ReLu was chosen for this model as it is widely used and relatively faster to compute. Softmax was used as the activation function in the output layer; it converts the output scores into probabilities for each class.

### Convolutional Neural Networks

Convolutional Neural Networks (CNNs) are an advanced class of neural network that comprise of three main layers: convolutional, pooling and fully connected, in addition to the input and output layers. The data undergoes transformations as it flows through the layers, and the complexity of the object increases. The convolutional layer is where most of the computation occurs, where filters are applied to the input to extract features. The pooling layer performs dimensionality reduction to reduce the number of parameters being passed on to the next layer. There are two main types of pooling: max pooling, where the filter selects the entry with the maximum value to send to the output array; and average pooling, where the filter computes the average value to send to the output array. Reducing the dimension aids with efficiency and reducing the risk of overfitting. The fully connected layer makes the final prediction based on features extracted in the previous layers (IBM, n.d.).

Similar to the NN, there were a number of hyperparameters to consider. Below are the chosen configurations:

-   Layer structure: The model used in this assignment had 1 input layer of embeddings with 50 nodes. There was also a one-dimensional convolutional layer with 64 filters. Max pooling for each 4 adjacent elements was utilised. A flattening layer was employed to reshape the data for the subsequent layers. Finally, another hidden layer with 32 nodes was incorporated before passing through the output node.

-   Number of epochs: The number of epochs was set to 30.

-   Dropout: A dropout ratio of 0.2 was used in the models.

-   Activation function: Similar to the NN model, the ReLu activation function was used in the hidden layers and softmax in the output layer.

## Results and Discussion

### Classification Trees

The first set of models are the bag-of-words, tf-idf and trigrams applied to the classification tree algorithm. Below are the test accuracies obtained.

```{r Classification Trees, warning=FALSE, message=FALSE, echo=FALSE}
#### Bag-of-Words ####
word_bag <- tidy_sona %>%
  filter(!word %in% stop_words$word) %>% 
  group_by(word) %>% 
  count() %>%
  ungroup() %>% 
  top_n(200, wt = n) %>%
  select(-n)

sona_tdf <- tidy_sona %>%
  inner_join(word_bag) %>%
  group_by(filename, president_13, word) %>%
  count() %>%  
  group_by(filename) %>%
  mutate(total = sum(n)) %>%
  ungroup()

bag_of_words <- sona_tdf %>% 
  select(filename, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  left_join(sona %>% select(filename, president_13)) %>%
  select(filename, president_13, everything())

# Building a classifier
set.seed(321)
sample_index <- createDataPartition(bag_of_words$president_13, p = 0.7, list = FALSE)
training_ids <- bag_of_words[sample_index, ] %>%
  select(filename)

# Decision Tree
training_sona <- bag_of_words %>%
  right_join(training_ids, by = 'filename') %>% 
  select(-filename)

test_sona <- bag_of_words %>%
  anti_join(training_ids, by = 'filename') %>% 
  select(-filename)

fit <- rpart(president_13 ~ ., data = training_sona, method = 'class')
predictions <- predict(fit, test_sona, type = 'class')
pred_test <- table(test_sona$president_13, predictions)
bow_acc <- round(sum(diag(pred_test))/sum(pred_test), 3) 

#### tf-idf ####
sona_tdf <- tidy_sona %>%
  inner_join(word_bag) %>%
  group_by(filename, president_13, word) %>%
  count() %>%  
  group_by(filename) %>%
  mutate(total = sum(n)) %>%
  ungroup()

ndocs <- length(unique(sona_tdf$filename))

# Calculate idf
idf <- sona_tdf %>% 
  group_by(word) %>% 
  summarize(docs_with_word = n()) %>% 
  ungroup() %>%
  mutate(idf = log(ndocs / docs_with_word)) %>% arrange(desc(idf))

# Calculate tf-idf
sona_tdf <- sona_tdf %>% 
  left_join(idf, by = 'word') %>% 
  mutate(tf = n/total, tf_idf = tf * idf)

# Reshape dataset
tfidf <- sona_tdf %>% 
  select(filename, word, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %>%  
  left_join(sona %>% select(filename, president_13))

# Building a classifier
set.seed(321)
sample_index <- createDataPartition(tfidf$president_13, p = 0.7, list = FALSE)
training_ids <- tfidf[sample_index, ] %>%
  select(filename)

# Decision Tree
training_sona <- tfidf %>% 
  right_join(training_ids, by = 'filename') %>%
  select(-filename)

test_sona <- tfidf %>% 
  anti_join(training_ids, by = 'filename') %>%
  select(-filename)

fit <- rpart(president_13 ~ ., data = training_sona, method = 'class')
predictions <- predict(fit, test_sona, type = 'class')
pred_test <- table(test_sona$president_13, predictions)
tfidf_acc <- round(sum(diag(pred_test))/sum(pred_test), 3)  

#### trigrams ####
sona_trigrams <- sona %>% 
  unnest_tokens(trigram, speech, token = "ngrams", n = 3)
trigrams_separated <- sona_trigrams %>%
  separate(trigram, c('word1','word2','word3'), sep = " ")
trigrams_filtered <- trigrams_separated %>%
  filter_at(vars(word1, word2, word3), all_vars(!(. %in% stop_words$word)))

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

top_trigrams <- trigrams_united %>%
  group_by(president_13, trigram) %>%
  count() %>%
  top_n(100, wt = n) %>%
  select(-n)

sona_tdf <- trigrams_united %>%
  inner_join(top_trigrams) %>%
  group_by(filename, president_13, trigram) %>%
  count() %>%
  group_by(filename) %>%
  mutate(total = sum(n)) %>%
  ungroup()

bag_of_t <- sona_tdf %>%
  select(filename, trigram, n) %>%
  pivot_wider(names_from = trigram, values_from = n, values_fill = 0) %>%
  left_join(sona %>% select(filename, president_13)) %>%
  select(filename, president_13, everything())

# Building a classifier
set.seed(321)
sample_index <- createDataPartition(bag_of_t$president_13, p = 0.7, list = FALSE)
training_ids <- bag_of_t[sample_index, ] %>% select(filename)

# Decision Tree
training_sona <- bag_of_t %>%
  right_join(training_ids, by = 'filename') %>% 
  select(-filename)

test_sona <- bag_of_t %>%
  anti_join(training_ids, by = 'filename') %>% 
  select(-filename)

fit <- rpart(president_13 ~ ., data = training_sona, method = 'class')
predictions <- predict(fit, test_sona, type = 'class')
pred_test <- table(test_sona$president_13, predictions)
tri_acc <- round(sum(diag(pred_test))/sum(pred_test), 3)

decision_trees <- data.frame(Model=c("Bag-of-words","tf-idf","Trigram"), Accuracy=c(bow_acc, tfidf_acc, tri_acc))
kbl(decision_trees, caption = "Classification Tree Model Test Accuracies") %>% kable_styling(bootstrap_options = c("hover","striped"),latex_options = "HOLD_position")
```

The accuracies are the same for all three models, indicating that regardless of the feature representation, the classification trees do a poor job at predicting the president. Below the graphical representation of the decision tree with trigrams is presented. It can be seen that the only predictions made were Zuma and Mbeki. This further confirms that the model was unable to perform the classification task.

```{r Classification Tree Plot, warning=FALSE, message=FALSE, echo=FALSE}
rpart.plot(fit, main="Classification tree applied on the trigram model")
```

### Neural Networks

The NN models were fit on the bag-of-words, tf-idf and trigrams models. Plots with the training and validation losses and accuracies of each one are presented below along with the test accuracies. A seed was set for reproducibility. 

```{r NN BOW, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="NN Performance on Bag-of-Words"}
set.seed(321)
sample_index <- createDataPartition(bag_of_words$president_13, p = 0.7, list = FALSE)
training_ids <- bag_of_words[sample_index, ] %>%
  select(filename)

training_rows <- sample_index

train <- list()
test <- list()
train$x <- as.matrix(bag_of_words[training_rows,-c(1,2)])
test$x <-  as.matrix(bag_of_words[-training_rows,-c(1,2)])

train$y <- to_categorical(as.integer(bag_of_words$president_13[training_rows]) - 1)
test$y <-  to_categorical(as.integer(bag_of_words$president_13[-training_rows]) - 1)

tensorflow::set_random_seed(4)
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = dim(train$x)[2], activation = "relu") %>% 
  layer_dropout(rate=0.2) %>% 
  layer_dense(units = 6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

rub1 <- capture.output({
  history <- model %>% fit(train$x, train$y, epochs = 65, 
                         batch_size = 20,  validation_split = 0.1, verbose = 0) })

plot(history)

results_bow <- model %>% evaluate(test$x, test$y, batch_size = 20, verbose = 2)
```

```{r NN tf-idf, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="NN Performance on tf-idf"}
set.seed(321)
sample_index <- createDataPartition(tfidf$president_13, p = 0.7, list = FALSE)
training_ids <- tfidf[sample_index, ] %>%
  select(filename)
training_rows <- sample_index

train <- list()
test <- list()
train$x <- as.matrix(tfidf[training_rows,-c(1,202)])
test$x <-  as.matrix(tfidf[-training_rows,-c(1,202)])

train$y <- to_categorical(as.integer(tfidf$president_13[training_rows]) - 1)
test$y <-  to_categorical(as.integer(tfidf$president_13[-training_rows]) - 1)

tensorflow::set_random_seed(4)
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = dim(train$x)[2], activation = "relu") %>% 
  layer_dropout(rate=0.2) %>% 
  layer_dense(units = 6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

rub2 <- capture.output({
  history <- model %>% fit(train$x, train$y, epochs = 65, 
                         batch_size = 20,  validation_split = 0.1, verbose = 0) })
plot(history)

results_tfidf <- model %>% evaluate(test$x, test$y, batch_size = 20, verbose = 2)
```

```{r NN Trigrams, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="NN Performance on Trigrams"}
set.seed(321)
sample_index <- createDataPartition(bag_of_t$president_13, p = 0.7, list = FALSE)
training_ids <- bag_of_t[sample_index, ] %>% select(filename)

training_rows <- sample_index

train <- list()
test <- list()
train$x <- as.matrix(bag_of_t[training_rows,-c(1,2)])
test$x <-  as.matrix(bag_of_t[-training_rows,-c(1,2)])

train$y <- to_categorical(as.integer(bag_of_t$president_13[training_rows]) - 1)
test$y <-  to_categorical(as.integer(bag_of_t$president_13[-training_rows]) - 1)

tensorflow::set_random_seed(4)
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = dim(train$x)[2], activation = "relu") %>% 
  layer_dropout(rate=0.2) %>% 
  layer_dense(units = 6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

rub3 <- capture.output({
  history <- model %>% fit(train$x, train$y, epochs = 65, 
                         batch_size = 20,  validation_split = 0.1, verbose = 0) })
plot(history, main="NN Performance on Trigrams")

results_tri <- model %>% evaluate(test$x, test$y, batch_size = 20, verbose = 2)

neural_networks <- data.frame(Model=c("Bag-of-words","tf-idf","Trigram"), Accuracy=c(results_bow[2],results_tfidf[2], results_tri[2]))

kbl(neural_networks, caption = "Neural Network Models Test Accuracies") %>% kable_styling(bootstrap_options = c("hover","striped"),latex_options = "HOLD_position")
```

The bag-of-words model performed relatively well this time, yielding a test error rate of 70%, whereas the tf-idf model performed even more poorly than any of the decision trees, earning a test accuracy rate of 30%. This is also seen in the plots; the bag-of-words model did better at generalising, as its validation and training accuracies were quite similar. The loss of the tf-idf model was increasing and the cross-validation accuracies were consistently 0 which is a clear indication of its incapacity. The trigrams had a test accuracy of 70%, similar to the bag-of-words model, but the validation accuracies seem to have a more discrete distribution.

### Convolution Neural Network

A CNN using word embeddings as input was also fit and the results are presented below. A seed was once again set for reproducibility.

```{r CNN Model, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="CNN Performance with Word Embeddings"}
max_features <- 6000      
tokenizer = text_tokenizer(num_words = max_features)

fit_text_tokenizer(tokenizer, sona$speech)
sequences = tokenizer$texts_to_sequences(sona$speech)

y <- to_categorical(as.integer(sona$president_13) - 1)

set.seed(321)
sample_index <- createDataPartition(sona$president_13, p = 0.7, list = FALSE)

training_rows <- sample_index

train <- list()
test <- list()
train$x <- sequences[training_rows]
test$x <-  sequences[-training_rows]

train$y <- y[training_rows,]
test$y <-  y[-training_rows,]

#hist(unlist(lapply(sequences, length)), main = "Sequence length after tokenization")

maxlen <- 9000               
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)

embedding_dims <- 50
tensorflow::set_random_seed(4)
model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 4) %>%
  layer_flatten() %>%
  layer_dense(32, activation = "relu") %>%
  layer_dense(6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

rub4 <- capture.output({
  history <- model %>%
  fit(x_train, train$y, batch_size = 50, epochs = 30, verbose = 0, validation_split = 0.3)})
plot(history)
results <- model %>% evaluate(x_test, test$y, batch_size=50, verbose = 2)

kbl(data.frame(Accuracy=results[2]), caption = "CNN Model Test Accuracy",row.names = FALSE) %>% kable_styling(bootstrap_options = c("hover","striped"),latex_options = "HOLD_position")
```

The results show that the CNN model did not perform too well. The validation loss was increasing with every epoch and the test accuracy was 60%. The validation and training accuracies are not approximately equal which means that the model does not generalise well.

## Conclusion and Limitations

In this assignment, machine learning algorithms were implemented to attempt at predicting the president that made a certain statement. Classification trees, neural network and convolutional neural networks were applied on to bag-of-words, tf-idf and trigram models.
From the results, it was seen that the NN built using bag-of-words and trigram models yielded the highest test accuracies (70%), while the NN with tf-idf yielded the lowest (30%). 

It is important to note that there were various limitations to this study. One of them being that a hyperparameter search was not conducted with the NN and CNN models, due to computational constraints. When feasible, it is often advisable to perform a grid search that could potentially provide better performance results through considering different combinations of hyperparameter values.

Another limitation was not experimenting with resampling methods such as ROSE or SMOTE. Given the fact that deKlerk and Mothlanthe were severely underrepresented in the data, this might have affected the models' performative ability. Resampling methods are typically beneficial for addressing class imbalances, however they must be used with caution as they may lead to overfitting and loss of information.

## References

IBM. (n.d.). Decision Trees. Retrieved from https://www.ibm.com/topics/decision-trees

IBM. (n.d.). Convolutional Neural Networks. Retrieved from https://www.ibm.com/topics/convolutional-neural-networks

IBM. (n.d.). Neural Networks. Retrieved from https://www.ibm.com/topics/neural-networks

South African Government. (n.d.). State of the Nation Address. Retrieved from https://www.gov.za/state-nation-address. â€Œ
